# ========================================================================================
# Junjo AI Studio Environment Configuration
# ========================================================================================
#
# This file contains default settings for LOCAL DEVELOPMENT.
# For production deployment, see the comments below for required changes.
#
# Quick Start (Local Development):
#   1. Copy this file: cp .env.example .env
#   2. Generate secrets (BOTH with same command):
#      openssl rand -base64 32
#   3. Replace JUNJO_SESSION_SECRET and JUNJO_SECURE_COOKIE_KEY values below
#   4. Run: docker compose up -d
#   5. Access: http://localhost:5153
#
# Note: This repository uses pinned production images (image-based deployment mode).
# - JUNJO_BUILD_TARGET is not used in this deployment model. (defaults to production)
# - Local frontend access host and port: http://localhost:5153 (production build port)
# ========================================================================================

# === Sample Junjo App - Telemetry API Key =========================================================>
# This API key allows the sample Junjo App (junjo-app in docker compose) to send telemetry to the
# Junjo AI Studio instance. Setup an API key inside Junjo AI Studio and replace the default value. 
JUNJO_AI_STUDIO_API_KEY=junjo_ai_studio_api_key_here

# === Junjo AI Studio Environment Vars =============================================================>
# Junjo AI Studio's Running Environment:
# Value: development || production
# Implications for authentication and service linkage
# - Development: uses localhost and ports for services
# - Production: uses a production hostname and subdomains for services
JUNJO_ENV="development"

# === DEPLOYMENT ARCHITECTURE =============================================================>
# IMPORTANT: Backend and frontend MUST be on same domain for session cookies to work.
#
# SUPPORTED CONFIGURATIONS:
# ✅ Apex domain + subdomain:  example.com + api.example.com
# ✅ Subdomain + subdomain:    app.example.com + api.example.com
# ✅ Subdomain + apex:          api.example.com + example.com
# ❌ Cross-domain (WILL NOT WORK): app.example.com + service.run.app
#
# Why: Session cookies use SameSite=Strict for security (CSRF protection)
# See: docs/DEPLOYMENT.md for detailed setup instructions
# ==========================================================================================

# === PRODUCTION URL CONFIGURATION =============================================================>
# REQUIRED when JUNJO_ENV=production
#
# These values configure the public URLs where your Junjo AI Studio services are accessible.
# Frontend and Backend URLs MUST share the same registrable domain for session cookies to work (SameSite=Strict).
#
# Frontend URL (where users access the web UI)
# Example: https://app.example.com
# JUNJO_PROD_FRONTEND_URL=
#
# Backend API URL (where frontend sends HTTP requests)
# Must share same root domain as frontend (for session cookies)
# Example: https://api.example.com
# JUNJO_PROD_BACKEND_URL=
#
# Ingestion Service URL (REQUIRED)
# Where OpenTelemetry SDKs send telemetry data
# This is the public URL to your ingestion service container
# Your reverse proxy (Caddy, nginx) maps this to the internal gRPC port (50051)
# Example: https://ingestion.example.com
# JUNJO_PROD_INGESTION_URL=
#
# Example Production Configuration:
#   JUNJO_ENV=production
#   JUNJO_PROD_FRONTEND_URL=https://app.example.com
#   JUNJO_PROD_BACKEND_URL=https://api.example.com
#   JUNJO_PROD_INGESTION_URL=https://ingestion.example.com
# ==========================================================================================

# === Backend Vars =============================================================>
# Session Cookie Security (TWO KEYS REQUIRED for Python backend)
#
# IMPORTANT: Both keys MUST be base64-encoded strings (required by Fernet encryption).
#            Generate BOTH keys with the same command: openssl rand -base64 32
#
# 1. Session Secret (Signing - for integrity)
#    Provides INTEGRITY - prevents tampering with cookie data
#    REQUIRED FORMAT: Base64-encoded string
#    Generate with: openssl rand -base64 32
JUNJO_SESSION_SECRET=your_base64_secret_here
#
# 2. Secure Cookie Key (Encryption - for confidentiality)
#    Provides CONFIDENTIALITY - prevents reading cookie data
#    REQUIRED FORMAT: Base64-encoded 32-byte key (Fernet requires this exact format)
#    Generate with: openssl rand -base64 32
JUNJO_SECURE_COOKIE_KEY=your_base64_key_here

# Allowed Origins:
# A comma-separated list of allowed origins for CORS.
#
# IMPORTANT: Cannot use "*" because session cookies require credentials=True,
#            which mandates specific origins (browser security policy).
#
# DEVELOPMENT: Defaults to http://localhost:5151,http://localhost:5153
#              (covers both dev and prod build ports)
# PRODUCTION: Auto-derived from JUNJO_PROD_FRONTEND_URL if not set
#
# You can explicitly set this to override defaults or support multiple frontends:
# Example: JUNJO_ALLOW_ORIGINS=https://app.example.com,https://admin.example.com
# JUNJO_ALLOW_ORIGINS=http://localhost:5151,http://localhost:5153

# Backend Server Port:
# Internal HTTP server port (inside Docker container).
# To change the external port, modify docker-compose.yml ports mapping.
# Example: "8080:1323" maps host port 8080 to container port 1323
JUNJO_BACKEND_PORT=1323

# === BACKEND MEMORY TUNING =====================================================================>
# Backend container memory guardrails (Docker Compose)
# These default settings are optimized for a low memory VM environment.
# Recommendation:
# - Ensure host swap is configured (for a 1GB VM, 1-2GB swap is a practical baseline).
# - memswap_limit controls TOTAL memory+swap available to the container.
# - For a 450m RAM limit with swap enabled, 900m total gives ~450m RAM + ~450m swap.
# - Set JUNJO_BACKEND_MEMSWAP_LIMIT=-1 for unlimited swap allowance.
# - If host swap is not configured, hitting JUNJO_BACKEND_MEM_LIMIT can still kill/restart
#   the backend container regardless of memswap settings.
#
# DEFAULT: 1GB VM profile:
# - Reservation: steady-state target
# - Limit: hard cap before container OOM-kill/restart
JUNJO_BACKEND_MEM_RESERVATION=300m
JUNJO_BACKEND_MEM_LIMIT=450m
JUNJO_BACKEND_MEMSWAP_LIMIT=900m
JUNJO_BACKEND_PIDS_LIMIT=128

# glibc allocator tuning (helps reduce RSS fragmentation in long-running processes)
JUNJO_MALLOC_ARENA_MAX=2
JUNJO_MALLOC_TRIM_THRESHOLD=131072

# DataFusion query runtime tuning (recommended for small VMs)
# 1 vCPU profile: keep parallelism low, batches moderate, spill enabled.
JUNJO_DF_TARGET_PARTITIONS=1
JUNJO_DF_BATCH_SIZE=4096
JUNJO_DF_PARQUET_PRUNING=true
JUNJO_DF_SPILL_ENABLED=true
JUNJO_DF_SPILL_POOL_MB=192
JUNJO_DF_SPILL_PATH=/tmp/junjo-datafusion-spill

# === DATABASE STORAGE CONFIGURATION =============================================================>
# Database Data Storage Location (HOST MACHINE PATH)
# This variable controls WHERE on your host machine the database files are stored.
#
# DEVELOPMENT (Local Docker):
#   JUNJO_HOST_DB_DATA_PATH=./.dbdata
#   - Stores databases in ./.dbdata directory (relative to project root)
#   - Git-ignored and persistent across container restarts
#   - Easy to delete/reset during development
#
# PRODUCTION (Mounted Block Storage):
#   JUNJO_HOST_DB_DATA_PATH=/mnt/junjo-data
#   - Stores databases on mounted persistent storage (DigitalOcean Volume, AWS EBS, etc.)
#   - Survives container restarts, redeployments, and even if containers are deleted
#
# PRODUCTION SETUP STEPS:
#   1. Create/mount block storage to a directory on your VM (e.g., /mnt/junjo-data)
#   2. Update this variable: JUNJO_HOST_DB_DATA_PATH=/mnt/junjo-data
#   3. Deploy with docker-compose up -d
#
# Block Storage Mount Examples:
#   - DigitalOcean:  mount /dev/disk/by-id/scsi-0DO_Volume_* /mnt/junjo-data
#   - AWS EC2:       mount /dev/xvdf /mnt/junjo-data
#   - Google Cloud:  mount /dev/disk/by-id/google-* /mnt/junjo-data
#
# Default: ./.dbdata (local development directory)
JUNJO_HOST_DB_DATA_PATH=./.dbdata

# === LOGGING CONFIGURATION ===================================================
# Log Level: debug, info, warn, error (default: info)
# Controls the minimum severity level for log output
# - debug: Show all logs including detailed diagnostics (use in development)
# - info:  Show informational messages and above (production default)
# - warn:  Show warnings and errors only
# - error: Show only error messages
JUNJO_LOG_LEVEL=info

# Log Format: json, text (default: json)
# Controls the output format of logs
# - json: Machine-readable JSON output (recommended for production)
# - text: Human-readable colored output (recommended for development)
JUNJO_LOG_FORMAT=text

# === AI SERVICE KEYS =============================================================================>
# Provider API Keys for LLM features (prompt playground)
# Uncomment and set the API keys for the providers you want to use

# Google Gemini API Key
# Get your API key from: https://aistudio.google.com/app/apikey
# GEMINI_API_KEY="your_gemini_api_key"

# OpenAI API Key
# Get your API key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY="your_openai_api_key"

# Anthropic API Key
# Get your API key from: https://console.anthropic.com/settings/keys
# ANTHROPIC_API_KEY="your_anthropic_api_key"

# === INGESTION SERVICE CONFIGURATION ========================================
#
# The Rust-based ingestion service uses a segmented WAL architecture:
#   Hot (Arrow IPC segments) -> Cold (Parquet)
#
# Data flow:
#   1. Spans arrive via gRPC, buffered in memory
#   2. When BATCH_SIZE reached (or 3s timer), written to IPC segment file
#   3. When total segment size reaches FLUSH_MAX_MB, all segments streamed to Parquet
#
# ==============================================================================

# ---------------------------------------------------------------------------
# BACKPRESSURE_MAX_MB - Memory safety valve
# ---------------------------------------------------------------------------
# Maximum process memory before rejecting new spans with RESOURCE_EXHAUSTED.
# OTLP clients automatically retry with exponential backoff.
#
# Tradeoffs:
#   Too low  -> Frequent backpressure, clients retry often, data delays
#   Too high -> Risk of OOM kill if container limit exceeded
#
# Rule of thumb: Set to ~60% of container memory limit
#
# Default: 300
# BACKPRESSURE_MAX_MB=300

# ---------------------------------------------------------------------------
# FLUSH_MAX_MB - IPC->Parquet flush threshold
# ---------------------------------------------------------------------------
# When total IPC segment size exceeds this, flush to Parquet is triggered.
#
# Tradeoffs:
#   Lower -> More frequent disk writes, more small Parquet files, lower peak memory
#   Higher -> Fewer disk writes, better compression ratio, higher peak memory
#
# This is an IO vs memory tradeoff. Lower values are safer for memory-constrained
# environments but create more Parquet files (slightly slower queries).
#
# Default: 25
# FLUSH_MAX_MB=25

# ---------------------------------------------------------------------------
# PREPARE_HOT_SNAPSHOT_CACHE_TTL_MS - Ingestion-side snapshot throttling
# ---------------------------------------------------------------------------
# The backend calls PrepareHotSnapshot per request (so it always receives
# up-to-date recent_cold_paths). Snapshot creation itself can be expensive
# when the WAL is large and the UI issues multiple concurrent requests.
#
# The ingestion service serializes snapshot creation and caches NON-EMPTY
# snapshots for a short TTL to avoid stampedes.
#
# Default: 1000
# PREPARE_HOT_SNAPSHOT_CACHE_TTL_MS=1000

# ---------------------------------------------------------------------------
# RECENT_COLD_MAX_FILES / RECENT_COLD_MAX_AGE_SECS - Flush->index query bridge
# ---------------------------------------------------------------------------
# When WAL data is flushed to a new cold Parquet file, the backend's SQLite
# metadata index may not have indexed that file yet. During that window, very
# new traces/workflows could be temporarily missing from SQLite-selected cold
# files and from the HOT snapshot (because the WAL may now be empty).
#
# To close that gap, ingestion returns a bounded list of recently flushed cold
# Parquet file paths as `recent_cold_paths` in the PrepareHotSnapshot response.
# The backend includes those paths as additional cold query sources until SQLite
# catches up.
#
# Defaults: 20 files, 120 seconds
# RECENT_COLD_MAX_FILES=20
# RECENT_COLD_MAX_AGE_SECS=120

# ---------------------------------------------------------------------------
# FLUSH_MAX_AGE_SECS - Time-based flush trigger
# ---------------------------------------------------------------------------
# Maximum time before forcing a flush even if size threshold not reached.
# Ensures data reaches Parquet files even during low-traffic periods.
#
# Default: 3600 (1 hour)
# FLUSH_MAX_AGE_SECS=3600

# ---------------------------------------------------------------------------
# BATCH_SIZE - Spans per IPC segment
# ---------------------------------------------------------------------------
# Maximum spans buffered in memory before writing to an IPC segment file.
# Also writes on 3-second timer if batch not full (for durability).
#
# Tradeoffs:
#   Lower -> More frequent small writes, lower latency to disk, more segment files
#   Higher -> Fewer writes, better batching efficiency, higher memory per batch
#
# Each segment is ~1-2MB depending on span attributes. More segments = more
# files for flusher to process, but lower memory usage per batch.
#
# Default: 1000
# BATCH_SIZE=1000


# === Caddy + Cloudflare DNS Vars ================================================================>
# This example uses Caddy as a reverse proxy, and Cloudflare as the DNS provider.
# The caddy/Dockerfile creates an xcaddy custom build with github.com/caddy-dns/cloudflare
#   - This plugin handles automated DNS challenge for Let's Encrypt certificates.
#   - This is not required for local development, but is required for production.
#
# These environment variables are used by Caddy to configure the Cloudflare plugin.
# If you are following this example and want to use Caddy's automatic SSL with a
# different DNS provider, you will need to configure Caddy accordingly.
#   - https://github.com/caddyserver/xcaddy
#   - https://caddyserver.com/docs/quick-starts/https
#
# This variable is created inside CloudFlare. See https://github.com/caddy-dns/cloudflare
CLOUDFLARE_API_TOKEN="your_api_token"

# === FOR SSL TESTING ============================================================================>
# Uncomment JUNJO_LETS_ENCRYPT_STAGING_CA_DIRECTIVE to use the Let's Encrypt staging environment.
# This lets you avoid production rate limits while you are testing or debugging your setup.
#
# See this repository's README.md for instructions on downloading and trusting
# the staging certificates on a MacOS system.
#
# Keep this line commented for production.
# JUNJO_LETS_ENCRYPT_STAGING_CA_DIRECTIVE="ca https://acme-staging-v02.api.letsencrypt.org/directory"
